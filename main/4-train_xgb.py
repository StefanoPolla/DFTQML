"""
Train a XGB model on a dataset of DFT energies and densities (DFTIO) for a specified input dataset.
Train and save a model for each of the 5 splits of the dataset (for 5-fold cross-validation).

The dataset is expected to be in .h5 format, as generated by the consolidation script
`3-consolidate_dat_to_h5.py`.
The trained models are saved in `./models/base/L{L}-N{N}-U{U}/{source}/ndata{ndata}`.
"""

import argparse
import os
import shutil
from dftqml import xgb, utils, data_processing

N_SPLITS = 5
DATA_DIR = "./data-h5"
MODEL_DIR = "./models/xgb"


# *** parse input ***
parser = argparse.ArgumentParser()

parser.add_argument("L", help="number of sites", type=int)
parser.add_argument("N", help="number of electrons", type=int)
parser.add_argument("U", help="coulomb repulsion", type=float)
parser.add_argument(
    "source", help="source data subdirectory (e.g. " "`vqe/depth1` or `exact`)", type=str
)
parser.add_argument("ndata", help="size of the dataset (train + val)", type=int)

parser.add_argument("--overwrite", help="overwrite existing ouput", action="store_true")

args = parser.parse_args()


# *** Manage data directories and load input ***
system_dir = f"L{args.L}-N{args.N}-U{args.U}"
input_dir = os.path.join(DATA_DIR, system_dir, args.source + ".h5")
output_dir = os.path.join(MODEL_DIR, system_dir, args.source, f"ndata{args.ndata}")

if not os.path.exists(input_dir):
    raise FileNotFoundError("the input directory does not exist")

densities, energies = utils.load_dft_data(input_dir, args.ndata)

# If loading data was successful, go on preparing output folder
if os.path.exists(output_dir):
    if args.overwrite:
        shutil.rmtree(output_dir)
    else:
        raise FileExistsError(
            f"{output_dir} exists. " "You can call the script with the --overwrite option."
        )

os.makedirs(output_dir, exist_ok=True)

# Split and amplify the data
x_train, y_train = data_processing.augment_by_shift_and_mirror(densities, energies)

# Train the model with 5-fold cross validation
model = xgb.initialize_xgb()
cv_model = xgb.fit_xgb(
    model, x_train, y_train, optimize_hyperparameters=True, set_shuffle=True, verbose=1
)

# Save the resulting model
xgb.save_xgb(cv_model, output_dir)

# Load the resulting model and its hyperparameters
# model = xgb.load_xgb(os.path.join(output_dir,"split0"))
