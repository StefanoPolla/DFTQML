'''
Train a CNN model on a dataset of DFT energies and densities (DFTIO) for a specified input dataset.
Train and save a model for each of the 5 splits of the dataset (for 5-fold cross-validation).

The dataset is expected to be in .h5 format, as generated by the consolidation script 
`3-consolidate_dat_to_h5.py`.
The trained models are saved in `./models/base/L{L}-N{N}-U{U}/{source}/ndata{ndata}`.
'''

import argparse
import os
import shutil

from sklearn import model_selection
from tensorflow import autograph
import logging

from dftqml import tfmodel, utils, data_processing


N_SPLITS = 5
DATA_DIR = './data-h5'
MODEL_DIR = './models/base'

# *** suppress autograph warnings ***

autograph.set_verbosity(0)
logging.getLogger("tensorflow").setLevel(logging.ERROR)


# *** parse input ***

parser = argparse.ArgumentParser()

parser.add_argument("L", help="number of sites", type=int)
parser.add_argument("N", help="number of electrons", type=int)
parser.add_argument("U", help="coulomb repulsion", type=float)
parser.add_argument("source", help="source data subdirectory (e.g. "
                                   "`vqe/depth1` or `exact`)", type=str)
parser.add_argument("ndata", help="size of the dataset (train + val)",
                    type=int)

parser.add_argument("--overwrite",
                    help="overwrite existing ouput",
                    action="store_true")

args = parser.parse_args()


# *** Manage data directories and load input ***

system_dir = f'L{args.L}-N{args.N}-U{args.U}'
input_dir = os.path.join(DATA_DIR, system_dir, args.source)
output_dir = os.path.join(MODEL_DIR, system_dir,
                          args.source, f'ndata{args.ndata}')

if not os.path.exists(input_dir):
    raise FileNotFoundError('the input directory does not exist at ' + input_dir)

densities, energies = utils.load_dft_data(input_dir, args.ndata)

# if loading data was successful, go on preparing output folder

if os.path.exists(output_dir):
    if args.overwrite:
        shutil.rmtree(output_dir)
    else:
        raise FileExistsError(
            f'{output_dir} exists. '
            'You can call the script with the --overwrite option.')

os.makedirs(output_dir, exist_ok=True)


# *** 5-fold cross validation ***

kfold_gen = model_selection.KFold(n_splits=N_SPLITS).split(densities, energies)

for k, (train_indices, val_indices) in enumerate(kfold_gen):

    # split and amplify data
    x_train, y_train = data_processing.augment_by_shift_and_mirror(densities[train_indices],
                                          energies[train_indices])
    x_val, y_val = data_processing.augment_by_shift_and_mirror(densities[val_indices],
                                      energies[val_indices])

    # Train model
    print(f'\n\n**** TRAINING MODEL AT SPLIT {k+1}/{N_SPLITS} ****\n\n')
    model = tfmodel.initialize_model(x_train, y_train)
    history = tfmodel.fit_history(model, x_train, y_train, x_val, y_val,
                                  batch_size=2 * args.L * 10, epochs=200,
                                  verbose=0, shuffle=True, patience=10)

    # Save result
    output_path = os.path.join(output_dir, f'split{k}')
    tfmodel.save_model(model, output_path, history_dict=history.history)
