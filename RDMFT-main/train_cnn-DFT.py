"""
Train a CNN model on a dataset of DFT energies and densities (DFTIO) for a specified input dataset.

Train and save a model for each of the 5 splits of the dataset (for 5-fold cross-validation).
The dataset is expected to be in .h5 format, as generated by the generate_instances script.
The trained models are saved in `./models/dftio/cnn/L{L}-N{N}-U{U}/ndata{ndata}`.
"""

import argparse
import os
import shutil

import h5py
from sklearn import model_selection
from tensorflow import autograph
import logging

from dftqml import tfmodel, utils


N_SPLITS = 5
DATA_DIR = "./data"
MODEL_DIR = "./models/dftio/cnn"

# *** suppress autograph warnings ***

autograph.set_verbosity(0)
logging.getLogger("tensorflow").setLevel(logging.ERROR)


# *** parse input ***

parser = argparse.ArgumentParser()

parser.add_argument("L", help="number of sites", type=int)
parser.add_argument("N", help="number of electrons", type=int)
parser.add_argument("U", help="coulomb repulsion", type=float)
# parser.add_argument("source", help="source file, e.g. L8-N8-U4.0.hdf5", type=str)
parser.add_argument("ndata", help="size of the dataset (train + val)", type=int)

parser.add_argument("--overwrite", help="overwrite existing ouput", action="store_true")

args = parser.parse_args()


# *** Manage data directories and load input ***

system_dir = f"L{args.L}-N{args.N}-U{args.U}"
input_file = os.path.join(DATA_DIR, system_dir + ".hdf5")
output_dir = os.path.join(MODEL_DIR, system_dir, f"ndata{args.ndata}")

if not os.path.exists(input_file):
    raise FileNotFoundError("the input directory does not exist at " + input_file)

with h5py.File(input_file, "r") as f:
    densities = f["densities"][:args.ndata]
    energies = f["dft_energies"][:args.ndata] # kinetic + interaction energy

# if loading data was successful, go on preparing output folder

if os.path.exists(output_dir):
    if args.overwrite:
        shutil.rmtree(output_dir)
    else:
        raise FileExistsError(
            f"{output_dir} exists. " "You can call the script with the --overwrite option."
        )

os.makedirs(output_dir, exist_ok=True)


# *** 5-fold cross validation ***

kfold_gen = model_selection.KFold(n_splits=N_SPLITS).split(densities, energies)

for k, (train_indices, val_indices) in enumerate(kfold_gen):
    # split and amplify data
    x_train, y_train = utils.augment_data(densities[train_indices], energies[train_indices])
    x_val, y_val = utils.augment_data(densities[val_indices], energies[val_indices])

    # Train model
    print(f"\n\n**** TRAINING MODEL AT SPLIT {k+1}/{N_SPLITS} ****\n\n")
    model = tfmodel.initialize_model(x_train, y_train)
    history = tfmodel.fit_history(
        model,
        x_train,
        y_train,
        x_val,
        y_val,
        batch_size=2 * args.L * 10,
        epochs=200,
        verbose=0,
        shuffle=True,
        patience=10,
    )

    # Save result
    output_path = os.path.join(output_dir, f"split{k}")
    tfmodel.save_model(model, output_path, history_dict=history.history)
