"""
Train a CNN model on a dataset of RDMFT energies and one-RDMs (RDMFTIO) for a specified input dataset.

Train and save a model for each of the 5 splits of the dataset (for 5-fold cross-validation).
The dataset is expected to be in .h5 format, as generated by the generate_instances script.
The trained models are saved in `./models/dftio/cnn/L{L}-N{N}-U{U}/ndata{ndata}`.
"""

import argparse
import os
import shutil

import h5py
from sklearn import model_selection
from tensorflow import autograph
import logging

from dftqml import tfmodel, data_processing


N_SPLITS = 5
SCRIPT_DIR = os.path.dirname(__file__)
DATA_DIR = os.path.join(SCRIPT_DIR, "data")
MODEL_DIR = os.path.join(SCRIPT_DIR, "models", "rdmftio")

# *** suppress autograph warnings ***

autograph.set_verbosity(0)
logging.getLogger("tensorflow").setLevel(logging.ERROR)


# *** parse input ***

parser = argparse.ArgumentParser()

parser.add_argument("L", help="number of sites", type=int)
parser.add_argument("N", help="number of electrons", type=int)
parser.add_argument("U", help="coulomb repulsion", type=float)
parser.add_argument("ndata", help="size of the dataset (train + val)", type=int)

parser.add_argument("--overwrite", help="overwrite existing ouput", action="store_true")

parser.add_argument(
    "--augment_by_permutations", help="augment data by permutations", action="store_true"
)
parser.add_argument(
    "--expand_one_rdm",
    help="expand one_rdm to include in each j-th column all correlators involving site j",
    action="store_true",
)

args = parser.parse_args()

if args.expand_one_rdm and args.augment_by_permutations:
    model_subdir = "cnn-expanded-augmented"
if args.expand_one_rdm:
    model_subdir = "cnn-expanded"
elif args.augment_by_permutations:
    model_subdir = "cnn-augmented"
else:
    model_subdir = "cnn"

# *** Manage data directories and load input ***

system_dir = f"L{args.L}-N{args.N}-U{args.U}"
input_file = os.path.join(DATA_DIR, system_dir + ".hdf5")
output_dir = os.path.join(MODEL_DIR, model_subdir, system_dir, f"ndata{args.ndata}")

if not os.path.exists(input_file):
    raise FileNotFoundError("the input directory does not exist at " + input_file)

with h5py.File(input_file, "r") as f:
    one_rdms = f["one_rdms"][: args.ndata]
    rdmft_energies = f["rdmft_energies"][: args.ndata]  # kinetic + interaction energy

# transpose RDMs so the locality index is last, as expected by the data augmentation and model
one_rdms = one_rdms.transpose((0, 2, 1)) # TODO: change this directly in the data files

# if loading data was successful, go on preparing output folder
if os.path.exists(output_dir):
    if args.overwrite:
        shutil.rmtree(output_dir)
    else:
        raise FileExistsError(
            f"{output_dir} exists. " "You can call the script with the --overwrite option."
        )

os.makedirs(output_dir, exist_ok=True)


# *** 5-fold cross validation ***

kfold_gen = model_selection.KFold(n_splits=N_SPLITS).split(one_rdms, rdmft_energies)

for k, (train_indices, val_indices) in enumerate(kfold_gen):
    # split and augment data
    if args.augment_by_permutations:
        augment = data_processing.augment_by_permutations
    else:
        augment = data_processing.augment_by_shift_and_mirror

    x_train, y_train = augment(one_rdms[train_indices], rdmft_energies[train_indices])
    x_val, y_val = augment(one_rdms[val_indices], rdmft_energies[val_indices])

    # if requested, expand one_rdm correlators to include all local terms
    if args.expand_one_rdm:
        x_train = data_processing.one_rdm_compressed_to_all_correlators(x_train)
        x_val = data_processing.one_rdm_compressed_to_all_correlators(x_val)

    batch_size = 2 * args.L * 10

    # Train model
    print(f"\n\n**** TRAINING MODEL AT SPLIT {k+1}/{N_SPLITS} ****\n\n")
    model = tfmodel.initialize_model(x_train, y_train)
    history = tfmodel.fit_history(
        model,
        x_train,
        y_train,
        x_val,
        y_val,
        batch_size=batch_size,
        epochs=200,
        verbose=0,
        shuffle=True,
        patience=10,
    )

    # Save result
    output_path = os.path.join(output_dir, f"split{k}")
    tfmodel.save_model(model, output_path, history_dict=history.history)
